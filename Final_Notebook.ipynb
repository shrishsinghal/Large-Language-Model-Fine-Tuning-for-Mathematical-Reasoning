{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "Y7SoC81_XZKv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# This cell will take time\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdtL52OCXZKx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "max_seq_length = 2048 # Choose any\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Se2tU-IsXZKx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAxnlOo6XZKx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T02:19:31.568939Z",
     "iopub.status.busy": "2024-11-13T02:19:31.568549Z",
     "iopub.status.idle": "2024-11-13T02:19:33.171299Z",
     "shell.execute_reply": "2024-11-13T02:19:33.170402Z",
     "shell.execute_reply.started": "2024-11-13T02:19:31.568906Z"
    },
    "id": "Xv7fyGF3XZKx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# download and load competition dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
    "# print and see dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T02:19:57.819762Z",
     "iopub.status.busy": "2024-11-13T02:19:57.819323Z",
     "iopub.status.idle": "2024-11-13T02:19:57.826858Z",
     "shell.execute_reply": "2024-11-13T02:19:57.825860Z",
     "shell.execute_reply.started": "2024-11-13T02:19:57.819718Z"
    },
    "id": "y-3obQADXZKy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"### Example:\n",
    "\n",
    "Problem:\n",
    "Find the value of y if 4y - 7 = 9.\n",
    "\n",
    "Solution:\n",
    "Adding 7 to both sides, we get 4y = 16. Then, dividing by 4, we find y = 4.\n",
    "\n",
    "Answer:\n",
    "4\n",
    "\n",
    "Output:\n",
    "True\n",
    "\n",
    "Now, evaluate the following problem:\n",
    "\n",
    "### Problem:\n",
    "{}\n",
    "\n",
    "### Solution:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\n",
    "\n",
    "### Output:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    question = examples[\"question\"]\n",
    "    ans       = examples[\"answer\"]\n",
    "    output      = examples[\"is_correct\"]\n",
    "    explaination = examples[\"solution\"] #my edit\n",
    "    texts = []\n",
    "    for instruction, input_explaination, input, output in zip(question, explaination, ans, output):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = prompt.format(instruction, input_explaination, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T02:20:03.765432Z",
     "iopub.status.busy": "2024-11-13T02:20:03.764757Z",
     "iopub.status.idle": "2024-11-13T02:20:03.814272Z",
     "shell.execute_reply": "2024-11-13T02:20:03.813356Z",
     "shell.execute_reply.started": "2024-11-13T02:20:03.765371Z"
    },
    "id": "xHb9bNJ5XZKy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample 110,000 random rows from the original dataset\n",
    "sampled_data = dataset['train'].shuffle(seed=3407).select(range(110000))\n",
    "\n",
    "# Split the sampled data into 100,000 for training and 10000 for testing\n",
    "split_data = sampled_data.train_test_split(test_size=10000, seed=3407)\n",
    "\n",
    "# Separate into training and validation datasets\n",
    "train_data = split_data['train']\n",
    "val_dataset = split_data['test']\n",
    "\n",
    "# Format the training dataset with the prompt function\n",
    "train_dataset = train_data.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDoT2XL7XZKy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#print a smaple training example\n",
    "train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gp6EWbiXZKz"
   },
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeruJei_XZKz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 5000,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seEFyowgXZKz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLbhKbKOXZKz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knlsRNlKYUIb"
   },
   "outputs": [],
   "source": [
    "!zip -r lora_model.zip lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fah12Hx-YULF"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"lora_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaDlfVCDXZK0"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T02:20:17.217458Z",
     "iopub.status.busy": "2024-11-13T02:20:17.216382Z",
     "iopub.status.idle": "2024-11-13T02:20:17.222583Z",
     "shell.execute_reply": "2024-11-13T02:20:17.221587Z",
     "shell.execute_reply.started": "2024-11-13T02:20:17.217387Z"
    },
    "id": "Yv2ghzkaXZK0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfjaPdwdzkWr"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the .zip file\n",
    "zip_file_path = \"/content/lora_model.zip\"  # Replace with your file name\n",
    "\n",
    "# Extract the .zip file\n",
    "extracted_folder = \"/content\"  # Folder to extract the contents\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)\n",
    "\n",
    "print(f\"Model extracted to: {extracted_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T02:18:59.524276Z",
     "iopub.status.busy": "2024-11-13T02:18:59.523882Z",
     "iopub.status.idle": "2024-11-13T02:19:12.029175Z",
     "shell.execute_reply": "2024-11-13T02:19:12.028223Z",
     "shell.execute_reply.started": "2024-11-13T02:18:59.524237Z"
    },
    "id": "khcbtzuuXZK1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3nsDCJtaTw6"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "batch_size = 32  # Adjust this value based on available GPU memory\n",
    "test_dataset = dataset['test']\n",
    "num_samples = len(test_dataset['question'])  # Set dynamically based on dataset size\n",
    "\n",
    "\n",
    "# Pre-generate all input prompts for the test samples\n",
    "input_prompts = [\n",
    "    prompt.format(ques, ans, sol, \"\")  # \"\" for the output, allowing model to generate True/False\n",
    "    for ques, ans, sol in zip(test_dataset['question'], test_dataset['answer'], test_dataset['solution'])\n",
    "]\n",
    "\n",
    "# Prepare storage for responses and true labels\n",
    "responses = []\n",
    "true_labels = test_dataset['is_correct']  # Assumes 'is_correct' column contains boolean values\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    # Select batch of prompts\n",
    "    batch_prompts = input_prompts[i:i + batch_size]\n",
    "\n",
    "    # Tokenize and prepare inputs for model, move to GPU\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    input_shape = inputs['input_ids'].shape[1]  # The input token length for slicing output\n",
    "\n",
    "    # Run inference\n",
    "    outputs = model.generate(**inputs, max_new_tokens=5, use_cache=True)\n",
    "\n",
    "    # Decode each output starting from input length to capture only the generated part\n",
    "    batch_responses = tokenizer.batch_decode(\n",
    "        [output[input_shape:] for output in outputs], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Store the batch responses\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{(num_samples + batch_size - 1) // batch_size}\")\n",
    "\n",
    "print(\"Inference completed for all samples.\")\n",
    "\n",
    "# At this point, 'responses' contains all generated responses, and 'true_labels' has the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R92VKB8LaUAd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert generated responses to boolean values\n",
    "# Assumes `responses` is a list of lists with each inner list containing a single response string\n",
    "# Convert to boolean by checking if it's \"True\"\n",
    "\n",
    "predictions = [resp.strip() == 'True' for resp in responses]\n",
    "\n",
    "# 1) Calculate accuracy\n",
    "accuracy = sum(pred == label for pred, label in zip(predictions, true_labels)) / len(predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# 2) Create a DataFrame and save to CSV\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV in Colab or Kaggle\n",
    "csv_path = 'predictions2.csv'\n",
    "df_output.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved as '{csv_path}' in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtFaz8nm0Xar"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGl8541yaUD1"
   },
   "outputs": [],
   "source": [
    "df_output['is_correct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYzeVzS0aUGl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
